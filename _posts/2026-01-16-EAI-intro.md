---
title: "具身智能技术路线入门"
date: 2026-01-16 
categories: [Embodied AI]
tags: [Embodied AI]
---

本文基于YunlongDong 的具身智能基础技术路线Talk进行总结

![figure1]({{ "/assets/img/intro2eai.png" | relative_url }})


## 检测分割

### SAM系列

* [SAM](https://github.com/facebookresearch/segment-anything):进行图片level的分割
* [SAM2](https://github.com/facebookresearch/sam2): 进行video level的分割
* [SAM3](https://github.com/facebookresearch/sam3):promptable segmentation in images and videos
  * 衍生产品: [SAM3D](https://github.com/facebookresearch/sam-3d-objects) 可以进行3D物品生成也可以进行人体生成有 **object** 和**body** 两个版本



## 数据

### Video

* [**Mimic Play**](https://github.com/j96w/MimicPlay)
![figure1]({{ "/assets/img/method_fig.png" | relative_url }})

* [**Vid2Robot**](https://vid2robot.github.io/)

<b><i style="color:#7b5aa6">更多paper可以参考(这个仓库好久没更新了后续fork完会持续更新)</i></b>:

*  https://github.com/H-Freax/Awesome-Video-Robotic-Papers 

优点：

- 最容易获取的数据源
- 海量且多样化的数据
- 以人类全速

缺点：

- 重建过程中存在巨大空白![s](https://vedder.io/img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png)和![a](https://vedder.io/img/compiled/state_of_robot_learning_dec_2025/7fb0675884892e8376989145c015d8ad.png)
- 状态可能不是第一人称视角，也可以是来自不同的视角角度，引入了较大的状态间隙
- 动作必须完全从原始数据中推断，通常通过 来自其他模型的伪标记过程（例如骨骼追踪器 / 人类手部追踪器)，容易误差累积
- 如果没有完整的人体景深，轨迹在运动动力学上可*不可*行，因为躯干会倾斜、重量转移， 伸手，等等



### 轻量级的硬件收集示范数据

#### 小型设备

* **ALOHA** : 双臂轻量收集数据设备
  ![figure1]({{ "/assets/img/ALOHA.png" | relative_url }})
* **GELLO** :
  ![figure1]({{ "/assets/img/GELLO.png" | relative_url }})

优点：

- 跟随机器人配备了完整的传感器套件，可以记录所有![s](https://vedder.io/img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png)
- 所有演示活动动力学上可行，就他们本来的样子 在机器人上执行

缺点：

- *通常比*人类慢得多（最多可达10倍！） 直接用手完成任务
- 操作员需要数周的练习才能熟练 足以让数据用于训练
- 需要现场配备全机器人收集数据——这非常重要 规模化采集的生产与资本需求

#### 手套型设备

**DexCap**:手套

**HIRO Hand** :套在手指上的设备来收集示范数据

优点：

- 操作员学习更快
- 更快的演示
- 规模化部署成本更低（例如通用型4, 星期日5)

缺点：

- 噪声重构 ![s](https://vedder.io/img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png) 和 ![一个](https://vedder.io/img/compiled/state_of_robot_learning_dec_2025/7fb0675884892e8376989145c015d8ad.png)，引入一个域间隙可能严重损害策略性能
- 本体感觉和作用需从SLAM推断末端执行器姿态估计
- 摄像机图像中，所有的都是人类手臂拿着装置，但在推断时间，机器人看到的是机器人手臂
- 运动动力学的可行性无法保证——人类可以伸手出去作为演示的一部分，或者用手臂达到机器人无法做到的姿势(人类的工作空间往往比机器人的工作空间大)

### 重量级的硬件收集

* **VR** 
* **外骨骼**

### 生成式仿真

* **RoboGen** 
* **Gen2Sim**
* **RoboTwin**
* **InternData-A1** 
* **MimicGen** 

这也是我最focus 的一个方向所以更多阅读paperlist会在后续开源

## 动作执行

### Imitation Learning

**ACT**:

**Diffusion Policy** :

$$\pi$$ 系列: $$\pi 0$$ ,$$\pi_{0.5}$$ ,$$\pi_{0.6}$$ 

**缺点:** 

* *out of distribution* states :
  * 光照等环境变量的突然改变就会导致prediction改变
  * partially observation 会导致动作偏移
  * 多义性动作比如你绕柱子可以从左边绕也可以从右边绕没有保证.
  * prediction error 的accumulation 导致最后完全不像样

**DAgger**:
![figure1]({{ "/assets/img/DAGGER.png" | relative_url }})


### Affordance:检测物品的可操作部分

**RoboAffordance**:

**AffordPose** :

**SceneFun3D** :

<b><i style="color:#7b5aa6">更多paper可以参考</i></b>: https://github.com/hq-King/Awesome-Affordance-Learning

### 大模型的应用

#### 利用大模型的QA来采取action

**ManipLLM** :

**ManipVQA** :

#### 大模型的planning 能力

### World Model

**3D VLA** 

**LAPO** 



# 高质量Paper关注list

**高质量会议与期刊（论文检索时重点关注）**
Science Robotics, TRO, IJRR, JFR, RSS, RAL, IROS, ICRA, ICCV, ECCV, ICML, CVPR, NeurIPS, CoRL, ICLR, AAAI, ACL

**长期跟进研究进展与选题调研**

- Awesome Humanoid Robot Learning（Yanjie Ze）：[repo](https://github.com/YanjieZe/awesome-humanoid-robot-learning)
- Paper Reading List（DeepTimber Community）：[repo](https://github.com/DeepTimber-Robot-Lab/Paper-Reading-List)
- Paper List（Yanjie Ze）：[repo](https://github.com/YanjieZe/Paper-List)
- RoboScholar / Embodied AI Paper List（Tianxing Chen）：[repo](https://github.com/TianxingChen/Paper-List-For-EmbodiedAI)
- SOTA Paper Rating（Weiyang Jin）：[website](https://waynejin0918.github.io/SOTA-paper-rating.io/)
- Awesome LLM Robotics：[repo](https://github.com/GT-RIPL/Awesome-LLM-Robotics)
- Awesome Video Robotic Papers：[repo](https://github.com/H-Freax/Awesome-Video-Robotic-Papers)
- Awesome Embodied Robotics and Agent：[repo](https://github.com/zchoi/Awesome-Embodied-Robotics-and-Agent)
- awesome-embodied-vla / va / vln：[repo](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln)
- Awesome Affordance Learning：[repo](https://github.com/hq-King/Awesome-Affordance-Learning)
- Embodied AI Paper TopConf：[repo](https://github.com/Songwxuan/Embodied-AI-Paper-TopConf)
- Awesome **RL-VLA** for Robotic Manipulation (Haoyuan Deng)：[repo](https://github.com/Denghaoyuan123/Awesome-RL-VLA)
