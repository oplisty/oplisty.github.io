---
title: "CS231n Homework 1"
date: 2026-01-21
categories: [Computer vision]
tags: [Computer vision]
math: true
---

### Inline Question 1

Notice the structured patterns in the distance matrix, where some rows or columns are visibly brighter.  
(Note that with the default color scheme black indicates low distances while white indicates high distances.)

- What in the data is the cause behind the distinctly bright rows?
- What causes the columns?

<span style="color: #1e6bd6;"><em>Your Answer:</em></span>

1. The bright rows indicate large distances between a test sample and **all** training samples, which usually means the test sample is very different from the training set (e.g., an outlier or unusual example).

2. The bright columns indicate large distances between a particular training sample and **all** test samples, which usually means that training sample is an outlier or otherwise atypical.

---

### Inline Question 2

**Inline Question 2**

We can also use other distance metrics such as L1 distance.
For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$,

the mean $\mu$ across all pixels over all images is $$\mu=\frac{1}{nhw}\sum_{k=1}^n\sum_{i=1}^{h}\sum_{j=1}^{w}p_{ij}^{(k)}$$
And the pixel-wise mean $\mu_{ij}$ across all images is
$$\mu_{ij}=\frac{1}{n}\sum_{k=1}^np_{ij}^{(k)}.$$
The general standard deviation $\sigma$ and pixel-wise standard deviation $\sigma_{ij}$ is defined similarly.

Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply. To clarify, both training and test examples are preprocessed in the same way.

1. Subtracting the mean $\mu$ ($\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu$.)
2. Subtracting the per pixel mean $\mu_{ij}$  ($\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu_{ij}$.)
3. Subtracting the mean $\mu$ and dividing by the standard deviation $\sigma$.
4. Subtracting the pixel-wise mean $\mu_{ij}$ and dividing by the pixel-wise standard deviation $\sigma_{ij}$.
5. Rotating the coordinate axes of the data, which means rotating all the images by the same angle. Empty regions in the image caused by rotation are padded with a same pixel value and no interpolation is performed.

<span style="color: #1e6bd6;"><em>Your Answer:</em></span>

- **(1) and (2)**

<span style="color: #1e6bd6;"><em>Your Explanation:</em></span>

For L1 distance, subtracting a constant shift applied to both training and test data does not change pairwise distances:

- For (1): \(lVert (x-\mu) - (y-\mu) \rVert_1 = \lVert x-y \rVert_1\).
- For (2): \(\lVert (x-\mu_{ij}) - (y-\mu_{ij}) \rVert_1 = \lVert x-y \rVert_1\).

Options (3) and (4) involve scaling (globally or per-pixel), which changes the weighting of dimensions and can change nearest neighbors.  
Option (5) changes the coordinate arrangement of pixel values and introduces padding regions, so it can also change L1 distances and nearest neighbors.

#### 额外的代码复杂度分析

```python
 def compute_distances_two_loops(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using a nested loop over both the training data and the
        test data.

        Inputs:
        - X: A numpy array of shape (num_test, D) containing test data.

        Returns:
        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
          is the Euclidean distance between the ith test point and the jth training
          point.
        """
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        for i in range(num_test):
            for j in range(num_train):
                dists[i][j]= np.sqrt(np.sum((X[i]-self.X_train[j]) ** 2))
        return dists

    def compute_distances_one_loop(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using a single loop over the test data.

        Input / Output: Same as compute_distances_two_loops
        """
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        for i in range(num_test):
            diff=self.X_train-X[i]
            dists[i,:]=np.sqrt(np.sum(diff**2,axis=1))
            #######################################################################
            #                                                               #
            # Compute the l2 distance between the ith test point and all training #
            # points, and store the result in dists[i, :].                        #
            # Do not use np.linalg.norm().                                        #
            #######################################################################
            
        return dists

    def compute_distances_no_loops(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using no explicit loops.

        Input / Output: Same as compute_distances_two_loops
        """
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        X_squre=np.sum(X**2,axis=1).reshape(num_test,1)
        X_time=np.ones((1,num_train))
        X_train_time=np.ones((1,num_test))
        X_train_squre=np.sum(self.X_train**2,axis=1).reshape(num_train ,1)
        cross=X @ self.X_train.T
        dists=np.sqrt(np.maximum(X_squre @ X_time + X_train_time.T @ X_train_squre.T -cross *2,0.0))
        #########################################################################
        #                                                                #
        # Compute the l2 distance between all test points and all training      #
        # points without using any explicit loops, and store the result in      #
        # dists.                                                                #
        #                                                                       #
        # You should implement this function using only basic array operations; #
        # in particular you should not use functions from scipy,                #
        # nor use np.linalg.norm().                                             #
        #                                                                       #
        # HINT: Try to formulate the l2 distance using matrix multiplication    #
        #       and two broadcast sums.                                         #
        #########################################################################

        return dists

```

* `two_loop`: 时间复杂度 Θ(N⋅M⋅D)

* `one_loop`:时间复杂度 Θ(N⋅M⋅D)
  * **渐进时间复杂度和 two-loops 一样**，但通常更快，因为 NumPy 内部用 C 实现，减少 Python 层循环开销。

* `no_loop`:时间复杂度Θ(N⋅M⋅D)
  * **渐进复杂度仍然一样**，但通常是最快的：因为把大量运算交给 BLAS/底层矩阵乘法实现，CPU 向量化、缓存友好，并且几乎没有 Python 循环开销。



