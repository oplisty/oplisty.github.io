---
title: "CS231n Homework 1"
date: 2026-01-22
categories: [Computer vision]
tags: [Computer vision]
math: true
---

# KNN

**Inline Question 1** 

Notice the structured patterns in the distance matrix, where some rows or columns are visibly brighter.  
(Note that with the default color scheme black indicates low distances while white indicates high distances.)

- What in the data is the cause behind the distinctly bright rows?
- What causes the columns?

<span style="color: #1e6bd6;"><em>Your Answer:</em></span>

1. The bright rows indicate large distances between a test sample and **all** training samples, which usually means the test sample is very different from the training set (e.g., an outlier or unusual example).

2. The bright columns indicate large distances between a particular training sample and **all** test samples, which usually means that training sample is an outlier or otherwise atypical.



**Inline Question 2**

We can also use other distance metrics such as L1 distance.
For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$,

the mean $\mu$ across all pixels over all images is $$\mu=\frac{1}{nhw}\sum_{k=1}^n\sum_{i=1}^{h}\sum_{j=1}^{w}p_{ij}^{(k)}$$
And the pixel-wise mean $\mu_{ij}$ across all images is
$$\mu_{ij}=\frac{1}{n}\sum_{k=1}^np_{ij}^{(k)}.$$
The general standard deviation $\sigma$ and pixel-wise standard deviation $\sigma_{ij}$ is defined similarly.

Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply. To clarify, both training and test examples are preprocessed in the same way.

1. Subtracting the mean $\mu$ 
2. Subtracting the per pixel mean $\mu_{ij}$  
3. Subtracting the mean $\mu$ and dividing by the standard deviation $\sigma$.
4. Subtracting the pixel-wise mean $\mu_{ij}$ and dividing by the pixel-wise standard deviation $\sigma_{ij}$.
5. Rotating the coordinate axes of the data, which means rotating all the images by the same angle. Empty regions in the image caused by rotation are padded with a same pixel value and no interpolation is performed.

<span style="color: #1e6bd6;"><em>Your Answer:</em></span>

- **(1) and (2)**

<span style="color: #1e6bd6;"><em>Your Explanation:</em></span>



**Inline Question 3**

Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.
1. The decision boundary of the k-NN classifier is linear.
2. The training error of a 1-NN will always be lower than or equal to that of 5-NN.
3. The test error of a 1-NN will always be lower than that of a 5-NN.
4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set.
5. None of the above.

$\color{blue}{\textit Your Answer:}$ 2, 4

$\color{blue}{\textit Your Explanation:}$​

**False.** k-NN 的决策边界一般是高度非线性的（尤其在高维/复杂分布下），只有在非常特殊的数据分布下才可能看起来接近线性。

**True.** 在训练集上，**1-NN** 对每个训练样本本身的最近邻就是它自己（距离为 0），因此训练集上可以做到训练误差为 0（假设没有完全重复但标签不同的冲突点）。

**False.**  1-NN 容易过拟合、方差大，5-NN 更平滑、泛化更好

**True.** 参考下面的代码复杂度分析

#### 额外的代码复杂度分析

```python
 def compute_distances_two_loops(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using a nested loop over both the training data and the
        test data.

        Inputs:
        - X: A numpy array of shape (num_test, D) containing test data.

        Returns:
        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
          is the Euclidean distance between the ith test point and the jth training
          point.
        """
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        for i in range(num_test):
            for j in range(num_train):
                dists[i][j]= np.sqrt(np.sum((X[i]-self.X_train[j]) ** 2))
        return dists

    def compute_distances_one_loop(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using a single loop over the test data.

        Input / Output: Same as compute_distances_two_loops
        """
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        for i in range(num_test):
            diff=self.X_train-X[i]
            dists[i,:]=np.sqrt(np.sum(diff**2,axis=1))
            #######################################################################
            #                                                               #
            # Compute the l2 distance between the ith test point and all training #
            # points, and store the result in dists[i, :].                        #
            # Do not use np.linalg.norm().                                        #
            #######################################################################
            
        return dists

    def compute_distances_no_loops(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train using no explicit loops.

        Input / Output: Same as compute_distances_two_loops
        """
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        X_squre=np.sum(X**2,axis=1).reshape(num_test,1)
        X_time=np.ones((1,num_train))
        X_train_time=np.ones((1,num_test))
        X_train_squre=np.sum(self.X_train**2,axis=1).reshape(num_train ,1)
        cross=X @ self.X_train.T
        dists=np.sqrt(np.maximum(X_squre @ X_time + X_train_time.T @ X_train_squre.T -cross *2,0.0))
        #########################################################################
        #                                                                #
        # Compute the l2 distance between all test points and all training      #
        # points without using any explicit loops, and store the result in      #
        # dists.                                                                #
        #                                                                       #
        # You should implement this function using only basic array operations; #
        # in particular you should not use functions from scipy,                #
        # nor use np.linalg.norm().                                             #
        #                                                                       #
        # HINT: Try to formulate the l2 distance using matrix multiplication    #
        #       and two broadcast sums.                                         #
        #########################################################################

        return dists

```

* `two_loop`: 时间复杂度 Θ(N⋅M⋅D)

* `one_loop`:时间复杂度 Θ(N⋅M⋅D)
  * **渐进时间复杂度和 two-loops 一样**，但通常更快，因为 NumPy 内部用 C 实现，减少 Python 层循环开销。

* `no_loop`:时间复杂度Θ(N⋅M⋅D)
  * **渐进复杂度仍然一样**，但通常是最快的：因为把大量运算交给 BLAS/底层矩阵乘法实现，CPU 向量化、缓存友好，并且几乎没有 Python 循环开销。



# SoftMax

**Inline Question 1**

Why do we expect our loss to be close to -log(0.1)? Explain briefly.**

$\color{blue}{\textit Your Answer:}$ **一开始模型对于任何类别都没有特殊的偏好,所以按理来说是均分的每个类别的可能性都是log(1/n)这里n=10,所以损失函数是-log(0.1)**

**Inline Question 2**

Although gradcheck is reliable softmax loss, it is possible that for SVM loss, once in a while, a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a svm loss gradient check could fail? How would change the margin affect of the frequency of this happening?

$\color{blue}{\textit Your Answer:}$ 

$\color{green}{\textit Discrepancy\  Reason:}$

the **multiclass SVM loss has “kinks”**: each term is a $\max(0, \cdot)$, so it’s **not differentiable at the point where the margin is exactly zero**:
$$
s_j - s_{y_i} + \Delta = 0
$$
A numerical gradient check uses finite differences, which can “step” from one side of the kink to the other, so the **numerical gradient may not match your analytic gradient** 

$\color{green}{\textit Is\ it\ a\ reason\ for\ concern?}$: 

Usually **no**. At the kink the gradient is not uniquely defined (there’s a **set of valid subgradients**), so a mismatch in *one dimension occasionally* is expected. If mismatches are frequent or huge everywhere, *then* it’s concerning.

$\color{green}{\textit Simple\ Example}$:   

Consider a 1D hinge loss:
$$
f(w) = \max(0, w)
$$
At $w=0$ it’s not differentiable.

- Analytic “gradient” (subgradient choice) might return $0$ (many implementations do).

- Numerical gradient with symmetric finite differences:
  $$
  \frac{f(h) - f(-h)}{2h} = \frac{h - 0}{2h} = \frac{1}{2}
  $$

which **doesn’t match** 0.

That’s exactly the kind of discrepancy you can see in SVM loss when some class satisfies
$$
s_j - s_{y_i} + \Delta \approx 0
$$
$\color{green}{\textit How\ does\ the\ margin\ \Delta\ affect\ how\ often\ this\ happens?}$:   

The kink happens when $s_j - s_{y_i} + \Delta = 0$. If you **increase $\Delta$**, you shift the hinge boundary, so **more constraints are near/over the boundary**, meaning you’re **more likely** to have some terms very close to 0 during training → **gradcheck mismatches become more frequent**.

If you **decrease $\Delta$** (toward 0), fewer terms sit near the boundary → **less frequent** kink-crossing in finite differences → **fewer** mismatches.

(Still, exact equality is measure-zero with real numbers; the practical issue is being *very close* to 0, where finite differences can cross the kink.)

**Inline question 3**

Describe what your visualized Softmax classifier weights look like, and offer a brief explanation for why they look the way they do.

$\color{blue}{\textit Your Answer:}$ **每个类别的weight可视化后和对应的类别平均图像都很像, 因为训练的时候我们的loss是算的他们权重与训练集像素的乘积作为score, 然后训练目标是最大化对应类别的score且最小化别的类别的score, 第i列✖️矩阵得到训练集对于类别i的分数, 所以会趋向于对像素出现在种类i比较多的值为1而不在i的像素表现为0 从而我们可视化的时候他会长得更像对应的类别**  

**Inline Question 4** - **True or False**

Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would change the softmax loss, but leave the SVM loss unchanged.

$\color{blue}{\textit Your Answer:}$*True* 

$\color{blue}{\textit Your Explanation:}$​​
Softmax loss is **never exactly zero** for a finite score gap, because it depends on the full probability distribution:
$$
L_i^{\text{softmax}} = -\log \frac{e^{s_{y_i}}}{\sum_j e^{s_j}}
$$

So even if the correct class already has the highest score by a lot, adding that datapoint still contributes a small (but positive) softmax loss.

SVM hinge loss **does become exactly zero** once the example is correctly classified with margin:
$$
L_i^{\text{svm}}=\sum_{j\ne y_i}\max(0, s_j - s_{y_i} + \Delta)
$$

If the new datapoint satisfies $s_{y_i} \ge s_j + \Delta$ for all $j\ne y_i$, then every term inside the max is negative, so the SVM loss contribution is **0**. Adding such a datapoint leaves the total SVM loss unchanged, while the softmax loss still increases by a tiny amount.

## 总结

1. 一个Model是怎么built的?
2. Batch_size怎么分割? 然后cross_validation怎么实现 ? 超参的网格化搜索
3. **怎么可视化学习到的矩阵对于不同class的表现**:

```python
w = best_softmax.W[:-1,:] # strip out the bias dim * num class
w = w.reshape(32, 32, 3, 10) 
w_min, w_max = np.min(w), np.max(w)
classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
for i in range(10):
    plt.subplot(2, 5, i + 1)

    # Rescale the weights to be between 0 and 255
    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)
    plt.imshow(wimg.astype('uint8'))
    plt.axis('off')
    plt.title(classes[i])
```

首先为什么是32，32，3，10的shape呢? 搞清楚训练的过程就很好懂了,每个matrix的参数对应哪个

# Two-Layer Neural Network

### Inline Question 1

We've only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?

1. Sigmoid
2. ReLU
3. Leaky ReLU

$\color{blue}{\textit Your Answer:}$ **Fill this in**
